{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regressor_network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZ91u9OiHg7tGyvxSZOndN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7Z96MTRPiK3A"},"source":["from tensorflow.keras.models import Model # Импортируем модели keras: Model\n","from tensorflow.keras.layers import * # Импортируем все слои keras\n","from tensorflow.keras import backend as K # Импортируем модуль backend keras'а\n","from tensorflow.keras.optimizers import Adam # Импортируем оптимизатор Adam\n","from tensorflow.keras import utils # Импортируем модуль utils библиотеки tensorflow.keras для получения OHE-представления\n","import tensorflow as tf\n","from google.colab import files # Импортируем Модуль files для работы с файлами\n","import matplotlib.pyplot as plt # Импортируем модуль pyplot библиотеки matplotlib для построения графиков\n","from tensorflow.keras.preprocessing import image # Импортируем модуль image для работы с изображениями\n","import numpy as np # Импортируем библиотеку numpy\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.applications import *\n","from tensorflow.keras.layers.experimental.preprocessing import Resizing\n","import time\n","import random\n","import shutil\n","import os # Импортируем библиотеку os для раоты с фаловой системой\n","from PIL import Image # импортируем модель Image для работы с изображениями\n","import seaborn as sns\n","\n","import numba as nb\n","import cv2 as cv\n","import json\n","from PIL import Image, ImageDraw\n","from numpy.lib.stride_tricks import as_strided\n","sns.set_style('darkgrid')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xSL_DUoNiMzo","executionInfo":{"status":"ok","timestamp":1618192647999,"user_tz":-180,"elapsed":7909,"user":{"displayName":"University of Artificial Intelligence","photoUrl":"","userId":"07499422860102443925"}},"outputId":"e2059e24-8b57-4adf-db93-d5b71753bc43"},"source":["from google.colab import drive # Подключаем гугл-диск\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8KPAbPbKiPrC"},"source":["directory = '/content/drive/My Drive/segmentation/' # Указываем путь к обучающей выборке с оригинальными изображения"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lKOruwZ_iV_4"},"source":["# Загружаем и обрабатываем данные"]},{"cell_type":"code","metadata":{"id":"OgsO92UkiaUR"},"source":["with open(directory + \"via_POLIGON_data.json\", \"r\") as write_file:\n","    data = json.load(write_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVc0BHQ2jAnX"},"source":["images = []\n","for filename in data.values():\n","    temp_img = np.array(Image.open(directory + 'orig/' + 'дог' + str(int(filename['filename'][0]) + 1) + '.jpg'))\n","    temp_img = cv.resize(temp_img, (768, 512), interpolation = cv.INTER_NEAREST)\n","    images.append(temp_img)\n","\n","\n","images = np.array(images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G1e3_9Gmi4Lp"},"source":["all_points = []\n","\n","for image_data in data.values():\n","  point = image_data['regions']['0']['shape_attributes']\n","  four_coords = tuple(zip(point['all_points_x'], point['all_points_y']))\n","  all_points.append(four_coords)\n","\n","all_points = np.array(all_points)[:, 0:4, :].astype(np.float32)\n","all_points[:, :, 0] = all_points[:, :, 0]/(images.shape[2] * 2)\n","all_points[:, :, 1] = all_points[:, :, 1]/(images.shape[1] * 2)\n","all_points = all_points.reshape(-1, 8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJ9h4nSYjZs6"},"source":["x_train, x_test, y_train, y_test = train_test_split(images, all_points, test_size = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5YWMAnqYlIUA"},"source":["# Создаем Собственную Нейросеть"]},{"cell_type":"code","metadata":{"id":"Wj1vTiFRlHP4"},"source":["def reslayer(*args, x, n = 5, **kwargs):\n","  y = Conv2D(padding = 'same', activation = 'relu', *args, **kwargs)(x)\n","  for i in range(n):\n","    y = Conv2D(padding = 'same', activation = 'relu', *args, **kwargs)(y)\n","    y = BatchNormalization()(y)\n","  y = Add()([x, y])\n","  return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5d7BKMHkDXM"},"source":["input_layer = Input((x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n","Conv_1 = Conv2D(32, (2, 2), padding = 'same', activation = 'relu')(input_layer)\n","\n","Conv_2 = Conv2D(64, (2, 2), strides = (2, 2), padding = 'same', activation = 'relu')(Conv_1)\n","Conv_2 = reslayer(64, (2, 2), x = Conv_2)\n","\n","Conv_3 = Conv2D(96, (2, 2), strides = (2, 2), padding = 'same', activation = 'relu')(Conv_2)\n","Conv_3 = reslayer(96, (2, 2), x = Conv_3)\n","\n","Conv_4 = Conv2D(128, (2, 2), strides = (2, 2), padding = 'same', activation = 'relu')(Conv_3)\n","Conv_4 = reslayer(128, (2, 2), x = Conv_4)\n","\n","Conv_5 = Conv2D(160, (2, 2), strides = (2, 2), padding = 'same', activation = 'relu')(Conv_4)\n","Conv_5 = reslayer(160, (2, 2), x = Conv_5)\n","\n","Conv_6 = Conv2D(180, (2, 2), strides = (2, 2), padding = 'same', activation = 'relu')(Conv_5)\n","Conv_6 = reslayer(180, (2, 2), x = Conv_6)\n","\n","Conv_7 = Conv2D(200, (2, 2), strides = (2, 2), padding = 'same', activation = 'relu')(Conv_6)\n","Conv_7 = reslayer(200, (2, 2), x = Conv_7)\n","\n","Conv_8 = Conv2D(256, (2, 2), strides = (2, 2), padding = 'same', activation = 'relu')(Conv_7)\n","Conv_8 = reslayer(256, (2, 2), x = Conv_8)\n","\n","flatten_layer = Flatten()(Conv_8)\n","dense1 = Dense(1024, activation = 'relu')(flatten_layer)\n","dense1 = Dropout(0.2)(dense1)\n","dense1 = BatchNormalization()(dense1)\n","dense2 = Dense(512, activation = 'relu')(dense1)\n","dense2 = Dropout(0.2)(dense2)\n","dense2 = BatchNormalization()(dense2)\n","\n","output_layer = Dense(8, activation = 'sigmoid')(dense2)\n","\n","model = Model(input_layer, output_layer)\n","\n","model.compile(optimizer=Adam(lr = 0.001), loss='binary_crossentropy', metrics=['mae'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ug53SfFJnnkj","executionInfo":{"status":"ok","timestamp":1618190075877,"user_tz":-180,"elapsed":8638,"user":{"displayName":"University of Artificial Intelligence","photoUrl":"","userId":"07499422860102443925"}},"outputId":"0f835958-9755-4d5a-c745-7603b00f1d37"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 512, 768, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 512, 768, 32) 416         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 256, 384, 64) 8256        conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 256, 384, 64) 16448       conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 256, 384, 64) 16448       conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 256, 384, 64) 256         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 256, 384, 64) 16448       batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 256, 384, 64) 256         conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 256, 384, 64) 16448       batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 256, 384, 64) 256         conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 256, 384, 64) 16448       batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 256, 384, 64) 256         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 256, 384, 64) 16448       batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 256, 384, 64) 256         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 256, 384, 64) 0           conv2d_1[0][0]                   \n","                                                                 batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 128, 192, 96) 24672       add[0][0]                        \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 128, 192, 96) 36960       conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 128, 192, 96) 36960       conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 128, 192, 96) 384         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 128, 192, 96) 36960       batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 128, 192, 96) 384         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 128, 192, 96) 36960       batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 128, 192, 96) 384         conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 128, 192, 96) 36960       batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 128, 192, 96) 384         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 128, 192, 96) 36960       batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 128, 192, 96) 384         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 128, 192, 96) 0           conv2d_8[0][0]                   \n","                                                                 batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 64, 96, 128)  49280       add_1[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 64, 96, 128)  65664       conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 64, 96, 128)  65664       conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 64, 96, 128)  512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 64, 96, 128)  65664       batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 64, 96, 128)  512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 64, 96, 128)  65664       batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 64, 96, 128)  512         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 64, 96, 128)  65664       batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 64, 96, 128)  512         conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 64, 96, 128)  65664       batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 64, 96, 128)  512         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 64, 96, 128)  0           conv2d_15[0][0]                  \n","                                                                 batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 32, 48, 160)  82080       add_2[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 32, 48, 160)  102560      conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 32, 48, 160)  102560      conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 32, 48, 160)  640         conv2d_24[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 32, 48, 160)  102560      batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 32, 48, 160)  640         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 32, 48, 160)  102560      batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 32, 48, 160)  640         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 32, 48, 160)  102560      batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 32, 48, 160)  640         conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 32, 48, 160)  102560      batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 32, 48, 160)  640         conv2d_28[0][0]                  \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 32, 48, 160)  0           conv2d_22[0][0]                  \n","                                                                 batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 16, 24, 180)  115380      add_3[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 16, 24, 180)  129780      conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 16, 24, 180)  129780      conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 16, 24, 180)  720         conv2d_31[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 16, 24, 180)  129780      batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 16, 24, 180)  720         conv2d_32[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 16, 24, 180)  129780      batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 16, 24, 180)  720         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 16, 24, 180)  129780      batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 16, 24, 180)  720         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 16, 24, 180)  129780      batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 16, 24, 180)  720         conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 16, 24, 180)  0           conv2d_29[0][0]                  \n","                                                                 batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 8, 12, 200)   144200      add_4[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 8, 12, 200)   160200      conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 8, 12, 200)   160200      conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 8, 12, 200)   800         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 8, 12, 200)   160200      batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 8, 12, 200)   800         conv2d_39[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 8, 12, 200)   160200      batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 8, 12, 200)   800         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 8, 12, 200)   160200      batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 8, 12, 200)   800         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 8, 12, 200)   160200      batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_29 (BatchNo (None, 8, 12, 200)   800         conv2d_42[0][0]                  \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 8, 12, 200)   0           conv2d_36[0][0]                  \n","                                                                 batch_normalization_29[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 4, 6, 256)    205056      add_5[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 4, 6, 256)    262400      conv2d_43[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 4, 6, 256)    262400      conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_30 (BatchNo (None, 4, 6, 256)    1024        conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 4, 6, 256)    262400      batch_normalization_30[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_31 (BatchNo (None, 4, 6, 256)    1024        conv2d_46[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 4, 6, 256)    262400      batch_normalization_31[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_32 (BatchNo (None, 4, 6, 256)    1024        conv2d_47[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 4, 6, 256)    262400      batch_normalization_32[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_33 (BatchNo (None, 4, 6, 256)    1024        conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 4, 6, 256)    262400      batch_normalization_33[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_34 (BatchNo (None, 4, 6, 256)    1024        conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 4, 6, 256)    0           conv2d_43[0][0]                  \n","                                                                 batch_normalization_34[0][0]     \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 6144)         0           add_6[0][0]                      \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 1024)         6292480     flatten[0][0]                    \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 1024)         0           dense[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_35 (BatchNo (None, 1024)         4096        dropout[0][0]                    \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 512)          524800      batch_normalization_35[0][0]     \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_36 (BatchNo (None, 512)          2048        dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 8)            4104        batch_normalization_36[0][0]     \n","==================================================================================================\n","Total params: 12,122,620\n","Trainable params: 12,108,708\n","Non-trainable params: 13,912\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dzNmFN5UnpaN","executionInfo":{"status":"ok","timestamp":1618191278330,"user_tz":-180,"elapsed":1211075,"user":{"displayName":"University of Artificial Intelligence","photoUrl":"","userId":"07499422860102443925"}},"outputId":"3b47b366-7343-430d-9c86-b9687fcf9cc8"},"source":["from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","cb1 = ModelCheckpoint(\n","    '/content/drive/My Drive/segmentation/weightsRegressor/',\n","    monitor=\"val_loss\",\n","    verbose=1,\n","    save_best_only=True,\n","    save_weights_only=True,\n","    mode=\"min\",\n","    save_freq=\"epoch\",\n","    options=None\n",")\n","\n","cb2 = ReduceLROnPlateau()\n","\n","history = model.fit(x_train/255, y_train, epochs=150, batch_size=16, validation_data = (x_test/255, y_test), callbacks = [cb1, cb2]) #  Обучаем модель на выборке по трем классам на полноразмерных изображениях"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","6/6 [==============================] - 30s 2s/step - loss: 0.8890 - mae: 0.4569 - val_loss: 0.6369 - val_mae: 0.4283\n","\n","Epoch 00001: val_loss improved from inf to 0.63685, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 2/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.8583 - mae: 0.4418 - val_loss: 0.5951 - val_mae: 0.4027\n","\n","Epoch 00002: val_loss improved from 0.63685 to 0.59507, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 3/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.8264 - mae: 0.4362 - val_loss: 0.5736 - val_mae: 0.3888\n","\n","Epoch 00003: val_loss improved from 0.59507 to 0.57361, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 4/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.7952 - mae: 0.4289 - val_loss: 0.5495 - val_mae: 0.3738\n","\n","Epoch 00004: val_loss improved from 0.57361 to 0.54948, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 5/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.7596 - mae: 0.4161 - val_loss: 0.5233 - val_mae: 0.3563\n","\n","Epoch 00005: val_loss improved from 0.54948 to 0.52333, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 6/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.7219 - mae: 0.4007 - val_loss: 0.5140 - val_mae: 0.3497\n","\n","Epoch 00006: val_loss improved from 0.52333 to 0.51399, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 7/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.6942 - mae: 0.3901 - val_loss: 0.5460 - val_mae: 0.3715\n","\n","Epoch 00007: val_loss did not improve from 0.51399\n","Epoch 8/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.6615 - mae: 0.3789 - val_loss: 0.4504 - val_mae: 0.3020\n","\n","Epoch 00008: val_loss improved from 0.51399 to 0.45043, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 9/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.6287 - mae: 0.3564 - val_loss: 0.4087 - val_mae: 0.2681\n","\n","Epoch 00009: val_loss improved from 0.45043 to 0.40874, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 10/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.5851 - mae: 0.3434 - val_loss: 0.3761 - val_mae: 0.2453\n","\n","Epoch 00010: val_loss improved from 0.40874 to 0.37606, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 11/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.5491 - mae: 0.3199 - val_loss: 0.3632 - val_mae: 0.2325\n","\n","Epoch 00011: val_loss improved from 0.37606 to 0.36317, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 12/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.5131 - mae: 0.3030 - val_loss: 0.3900 - val_mae: 0.2478\n","\n","Epoch 00012: val_loss did not improve from 0.36317\n","Epoch 13/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.4704 - mae: 0.2801 - val_loss: 0.3614 - val_mae: 0.2229\n","\n","Epoch 00013: val_loss improved from 0.36317 to 0.36140, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 14/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.4556 - mae: 0.2624 - val_loss: 0.3220 - val_mae: 0.1897\n","\n","Epoch 00014: val_loss improved from 0.36140 to 0.32201, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 15/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.3993 - mae: 0.2318 - val_loss: 0.3093 - val_mae: 0.1804\n","\n","Epoch 00015: val_loss improved from 0.32201 to 0.30927, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 16/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.3687 - mae: 0.2101 - val_loss: 0.2606 - val_mae: 0.1379\n","\n","Epoch 00016: val_loss improved from 0.30927 to 0.26057, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 17/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.3495 - mae: 0.1900 - val_loss: 0.2656 - val_mae: 0.1393\n","\n","Epoch 00017: val_loss did not improve from 0.26057\n","Epoch 18/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.3056 - mae: 0.1686 - val_loss: 0.2552 - val_mae: 0.1274\n","\n","Epoch 00018: val_loss improved from 0.26057 to 0.25522, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 19/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2930 - mae: 0.1530 - val_loss: 0.2076 - val_mae: 0.0776\n","\n","Epoch 00019: val_loss improved from 0.25522 to 0.20756, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 20/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2837 - mae: 0.1385 - val_loss: 0.2095 - val_mae: 0.0833\n","\n","Epoch 00020: val_loss did not improve from 0.20756\n","Epoch 21/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2575 - mae: 0.1207 - val_loss: 0.2063 - val_mae: 0.0790\n","\n","Epoch 00021: val_loss improved from 0.20756 to 0.20631, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 22/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2433 - mae: 0.1039 - val_loss: 0.1781 - val_mae: 0.0428\n","\n","Epoch 00022: val_loss improved from 0.20631 to 0.17814, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 23/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2377 - mae: 0.1014 - val_loss: 0.1760 - val_mae: 0.0363\n","\n","Epoch 00023: val_loss improved from 0.17814 to 0.17598, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 24/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2325 - mae: 0.0879 - val_loss: 0.1731 - val_mae: 0.0363\n","\n","Epoch 00024: val_loss improved from 0.17598 to 0.17311, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 25/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2222 - mae: 0.0801 - val_loss: 0.1712 - val_mae: 0.0325\n","\n","Epoch 00025: val_loss improved from 0.17311 to 0.17125, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 26/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2194 - mae: 0.0749 - val_loss: 0.1727 - val_mae: 0.0329\n","\n","Epoch 00026: val_loss did not improve from 0.17125\n","Epoch 27/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2070 - mae: 0.0693 - val_loss: 0.1742 - val_mae: 0.0344\n","\n","Epoch 00027: val_loss did not improve from 0.17125\n","Epoch 28/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2047 - mae: 0.0648 - val_loss: 0.1755 - val_mae: 0.0315\n","\n","Epoch 00028: val_loss did not improve from 0.17125\n","Epoch 29/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1980 - mae: 0.0623 - val_loss: 0.1776 - val_mae: 0.0309\n","\n","Epoch 00029: val_loss did not improve from 0.17125\n","Epoch 30/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2004 - mae: 0.0566 - val_loss: 0.1760 - val_mae: 0.0306\n","\n","Epoch 00030: val_loss did not improve from 0.17125\n","Epoch 31/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1824 - mae: 0.0541 - val_loss: 0.1762 - val_mae: 0.0308\n","\n","Epoch 00031: val_loss did not improve from 0.17125\n","Epoch 32/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1995 - mae: 0.0527 - val_loss: 0.1757 - val_mae: 0.0305\n","\n","Epoch 00032: val_loss did not improve from 0.17125\n","Epoch 33/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1919 - mae: 0.0490 - val_loss: 0.1762 - val_mae: 0.0305\n","\n","Epoch 00033: val_loss did not improve from 0.17125\n","Epoch 34/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2042 - mae: 0.0491 - val_loss: 0.1773 - val_mae: 0.0368\n","\n","Epoch 00034: val_loss did not improve from 0.17125\n","Epoch 35/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1834 - mae: 0.0443 - val_loss: 0.1818 - val_mae: 0.0425\n","\n","Epoch 00035: val_loss did not improve from 0.17125\n","Epoch 36/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1957 - mae: 0.0462 - val_loss: 0.1767 - val_mae: 0.0344\n","\n","Epoch 00036: val_loss did not improve from 0.17125\n","Epoch 37/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1787 - mae: 0.0437 - val_loss: 0.1757 - val_mae: 0.0327\n","\n","Epoch 00037: val_loss did not improve from 0.17125\n","Epoch 38/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1819 - mae: 0.0437 - val_loss: 0.1739 - val_mae: 0.0332\n","\n","Epoch 00038: val_loss did not improve from 0.17125\n","Epoch 39/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1858 - mae: 0.0450 - val_loss: 0.1733 - val_mae: 0.0336\n","\n","Epoch 00039: val_loss did not improve from 0.17125\n","Epoch 40/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1915 - mae: 0.0469 - val_loss: 0.1729 - val_mae: 0.0331\n","\n","Epoch 00040: val_loss did not improve from 0.17125\n","Epoch 41/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1918 - mae: 0.0466 - val_loss: 0.1729 - val_mae: 0.0327\n","\n","Epoch 00041: val_loss did not improve from 0.17125\n","Epoch 42/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1863 - mae: 0.0421 - val_loss: 0.1729 - val_mae: 0.0326\n","\n","Epoch 00042: val_loss did not improve from 0.17125\n","Epoch 43/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1992 - mae: 0.0452 - val_loss: 0.1721 - val_mae: 0.0322\n","\n","Epoch 00043: val_loss did not improve from 0.17125\n","Epoch 44/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1842 - mae: 0.0437 - val_loss: 0.1716 - val_mae: 0.0321\n","\n","Epoch 00044: val_loss did not improve from 0.17125\n","Epoch 45/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1844 - mae: 0.0443 - val_loss: 0.1711 - val_mae: 0.0320\n","\n","Epoch 00045: val_loss improved from 0.17125 to 0.17110, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 46/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1972 - mae: 0.0452 - val_loss: 0.1709 - val_mae: 0.0321\n","\n","Epoch 00046: val_loss improved from 0.17110 to 0.17090, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 47/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1920 - mae: 0.0456 - val_loss: 0.1707 - val_mae: 0.0318\n","\n","Epoch 00047: val_loss improved from 0.17090 to 0.17066, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 48/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1787 - mae: 0.0448 - val_loss: 0.1705 - val_mae: 0.0317\n","\n","Epoch 00048: val_loss improved from 0.17066 to 0.17053, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 49/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1950 - mae: 0.0464 - val_loss: 0.1706 - val_mae: 0.0316\n","\n","Epoch 00049: val_loss did not improve from 0.17053\n","Epoch 50/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.2025 - mae: 0.0488 - val_loss: 0.1705 - val_mae: 0.0320\n","\n","Epoch 00050: val_loss improved from 0.17053 to 0.17051, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 51/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1817 - mae: 0.0450 - val_loss: 0.1705 - val_mae: 0.0325\n","\n","Epoch 00051: val_loss did not improve from 0.17051\n","Epoch 52/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1925 - mae: 0.0437 - val_loss: 0.1704 - val_mae: 0.0321\n","\n","Epoch 00052: val_loss improved from 0.17051 to 0.17037, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 53/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1863 - mae: 0.0414 - val_loss: 0.1705 - val_mae: 0.0313\n","\n","Epoch 00053: val_loss did not improve from 0.17037\n","Epoch 54/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1803 - mae: 0.0412 - val_loss: 0.1708 - val_mae: 0.0310\n","\n","Epoch 00054: val_loss did not improve from 0.17037\n","Epoch 55/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1958 - mae: 0.0428 - val_loss: 0.1709 - val_mae: 0.0307\n","\n","Epoch 00055: val_loss did not improve from 0.17037\n","Epoch 56/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1968 - mae: 0.0449 - val_loss: 0.1706 - val_mae: 0.0307\n","\n","Epoch 00056: val_loss did not improve from 0.17037\n","Epoch 57/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1951 - mae: 0.0481 - val_loss: 0.1702 - val_mae: 0.0307\n","\n","Epoch 00057: val_loss improved from 0.17037 to 0.17023, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 58/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1862 - mae: 0.0404 - val_loss: 0.1702 - val_mae: 0.0307\n","\n","Epoch 00058: val_loss improved from 0.17023 to 0.17022, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 59/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1865 - mae: 0.0419 - val_loss: 0.1701 - val_mae: 0.0307\n","\n","Epoch 00059: val_loss improved from 0.17022 to 0.17010, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 60/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1907 - mae: 0.0474 - val_loss: 0.1701 - val_mae: 0.0310\n","\n","Epoch 00060: val_loss did not improve from 0.17010\n","Epoch 61/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1773 - mae: 0.0388 - val_loss: 0.1702 - val_mae: 0.0313\n","\n","Epoch 00061: val_loss did not improve from 0.17010\n","Epoch 62/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1922 - mae: 0.0463 - val_loss: 0.1704 - val_mae: 0.0316\n","\n","Epoch 00062: val_loss did not improve from 0.17010\n","Epoch 63/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1897 - mae: 0.0450 - val_loss: 0.1705 - val_mae: 0.0319\n","\n","Epoch 00063: val_loss did not improve from 0.17010\n","Epoch 64/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1912 - mae: 0.0458 - val_loss: 0.1705 - val_mae: 0.0324\n","\n","Epoch 00064: val_loss did not improve from 0.17010\n","Epoch 65/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1855 - mae: 0.0434 - val_loss: 0.1703 - val_mae: 0.0323\n","\n","Epoch 00065: val_loss did not improve from 0.17010\n","Epoch 66/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1925 - mae: 0.0449 - val_loss: 0.1702 - val_mae: 0.0327\n","\n","Epoch 00066: val_loss did not improve from 0.17010\n","Epoch 67/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1940 - mae: 0.0453 - val_loss: 0.1703 - val_mae: 0.0334\n","\n","Epoch 00067: val_loss did not improve from 0.17010\n","Epoch 68/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1893 - mae: 0.0422 - val_loss: 0.1701 - val_mae: 0.0332\n","\n","Epoch 00068: val_loss did not improve from 0.17010\n","Epoch 69/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1898 - mae: 0.0425 - val_loss: 0.1700 - val_mae: 0.0327\n","\n","Epoch 00069: val_loss improved from 0.17010 to 0.16998, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 70/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1762 - mae: 0.0395 - val_loss: 0.1697 - val_mae: 0.0322\n","\n","Epoch 00070: val_loss improved from 0.16998 to 0.16974, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 71/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1794 - mae: 0.0415 - val_loss: 0.1696 - val_mae: 0.0321\n","\n","Epoch 00071: val_loss improved from 0.16974 to 0.16963, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 72/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1854 - mae: 0.0393 - val_loss: 0.1696 - val_mae: 0.0321\n","\n","Epoch 00072: val_loss improved from 0.16963 to 0.16955, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 73/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1865 - mae: 0.0413 - val_loss: 0.1699 - val_mae: 0.0327\n","\n","Epoch 00073: val_loss did not improve from 0.16955\n","Epoch 74/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1846 - mae: 0.0423 - val_loss: 0.1703 - val_mae: 0.0337\n","\n","Epoch 00074: val_loss did not improve from 0.16955\n","Epoch 75/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1797 - mae: 0.0418 - val_loss: 0.1708 - val_mae: 0.0345\n","\n","Epoch 00075: val_loss did not improve from 0.16955\n","Epoch 76/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1843 - mae: 0.0413 - val_loss: 0.1711 - val_mae: 0.0349\n","\n","Epoch 00076: val_loss did not improve from 0.16955\n","Epoch 77/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1926 - mae: 0.0422 - val_loss: 0.1709 - val_mae: 0.0344\n","\n","Epoch 00077: val_loss did not improve from 0.16955\n","Epoch 78/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1848 - mae: 0.0398 - val_loss: 0.1707 - val_mae: 0.0340\n","\n","Epoch 00078: val_loss did not improve from 0.16955\n","Epoch 79/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1884 - mae: 0.0434 - val_loss: 0.1705 - val_mae: 0.0336\n","\n","Epoch 00079: val_loss did not improve from 0.16955\n","Epoch 80/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1927 - mae: 0.0439 - val_loss: 0.1706 - val_mae: 0.0336\n","\n","Epoch 00080: val_loss did not improve from 0.16955\n","Epoch 81/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1779 - mae: 0.0410 - val_loss: 0.1709 - val_mae: 0.0342\n","\n","Epoch 00081: val_loss did not improve from 0.16955\n","Epoch 82/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1852 - mae: 0.0375 - val_loss: 0.1708 - val_mae: 0.0341\n","\n","Epoch 00082: val_loss did not improve from 0.16955\n","Epoch 83/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1788 - mae: 0.0368 - val_loss: 0.1709 - val_mae: 0.0342\n","\n","Epoch 00083: val_loss did not improve from 0.16955\n","Epoch 84/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1867 - mae: 0.0439 - val_loss: 0.1710 - val_mae: 0.0344\n","\n","Epoch 00084: val_loss did not improve from 0.16955\n","Epoch 85/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1867 - mae: 0.0435 - val_loss: 0.1711 - val_mae: 0.0346\n","\n","Epoch 00085: val_loss did not improve from 0.16955\n","Epoch 86/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1906 - mae: 0.0420 - val_loss: 0.1711 - val_mae: 0.0346\n","\n","Epoch 00086: val_loss did not improve from 0.16955\n","Epoch 87/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1752 - mae: 0.0390 - val_loss: 0.1710 - val_mae: 0.0344\n","\n","Epoch 00087: val_loss did not improve from 0.16955\n","Epoch 88/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1847 - mae: 0.0423 - val_loss: 0.1708 - val_mae: 0.0341\n","\n","Epoch 00088: val_loss did not improve from 0.16955\n","Epoch 89/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1854 - mae: 0.0379 - val_loss: 0.1706 - val_mae: 0.0339\n","\n","Epoch 00089: val_loss did not improve from 0.16955\n","Epoch 90/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1856 - mae: 0.0409 - val_loss: 0.1706 - val_mae: 0.0340\n","\n","Epoch 00090: val_loss did not improve from 0.16955\n","Epoch 91/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1864 - mae: 0.0413 - val_loss: 0.1706 - val_mae: 0.0339\n","\n","Epoch 00091: val_loss did not improve from 0.16955\n","Epoch 92/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1805 - mae: 0.0411 - val_loss: 0.1704 - val_mae: 0.0337\n","\n","Epoch 00092: val_loss did not improve from 0.16955\n","Epoch 93/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1871 - mae: 0.0417 - val_loss: 0.1704 - val_mae: 0.0336\n","\n","Epoch 00093: val_loss did not improve from 0.16955\n","Epoch 94/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1876 - mae: 0.0397 - val_loss: 0.1703 - val_mae: 0.0334\n","\n","Epoch 00094: val_loss did not improve from 0.16955\n","Epoch 95/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1871 - mae: 0.0418 - val_loss: 0.1702 - val_mae: 0.0333\n","\n","Epoch 00095: val_loss did not improve from 0.16955\n","Epoch 96/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1824 - mae: 0.0400 - val_loss: 0.1702 - val_mae: 0.0332\n","\n","Epoch 00096: val_loss did not improve from 0.16955\n","Epoch 97/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1890 - mae: 0.0449 - val_loss: 0.1701 - val_mae: 0.0331\n","\n","Epoch 00097: val_loss did not improve from 0.16955\n","Epoch 98/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1861 - mae: 0.0446 - val_loss: 0.1699 - val_mae: 0.0329\n","\n","Epoch 00098: val_loss did not improve from 0.16955\n","Epoch 99/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1920 - mae: 0.0440 - val_loss: 0.1698 - val_mae: 0.0329\n","\n","Epoch 00099: val_loss did not improve from 0.16955\n","Epoch 100/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1835 - mae: 0.0391 - val_loss: 0.1698 - val_mae: 0.0328\n","\n","Epoch 00100: val_loss did not improve from 0.16955\n","Epoch 101/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1974 - mae: 0.0425 - val_loss: 0.1697 - val_mae: 0.0327\n","\n","Epoch 00101: val_loss did not improve from 0.16955\n","Epoch 102/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1868 - mae: 0.0428 - val_loss: 0.1696 - val_mae: 0.0325\n","\n","Epoch 00102: val_loss did not improve from 0.16955\n","Epoch 103/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1823 - mae: 0.0439 - val_loss: 0.1695 - val_mae: 0.0324\n","\n","Epoch 00103: val_loss improved from 0.16955 to 0.16951, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 104/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1832 - mae: 0.0389 - val_loss: 0.1695 - val_mae: 0.0324\n","\n","Epoch 00104: val_loss improved from 0.16951 to 0.16950, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 105/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1904 - mae: 0.0422 - val_loss: 0.1694 - val_mae: 0.0321\n","\n","Epoch 00105: val_loss improved from 0.16950 to 0.16936, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 106/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1794 - mae: 0.0380 - val_loss: 0.1694 - val_mae: 0.0321\n","\n","Epoch 00106: val_loss did not improve from 0.16936\n","Epoch 107/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1913 - mae: 0.0405 - val_loss: 0.1694 - val_mae: 0.0321\n","\n","Epoch 00107: val_loss did not improve from 0.16936\n","Epoch 108/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1828 - mae: 0.0385 - val_loss: 0.1694 - val_mae: 0.0321\n","\n","Epoch 00108: val_loss did not improve from 0.16936\n","Epoch 109/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1834 - mae: 0.0419 - val_loss: 0.1693 - val_mae: 0.0320\n","\n","Epoch 00109: val_loss improved from 0.16936 to 0.16929, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 110/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1895 - mae: 0.0452 - val_loss: 0.1693 - val_mae: 0.0320\n","\n","Epoch 00110: val_loss did not improve from 0.16929\n","Epoch 111/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1859 - mae: 0.0430 - val_loss: 0.1692 - val_mae: 0.0317\n","\n","Epoch 00111: val_loss improved from 0.16929 to 0.16920, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 112/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1860 - mae: 0.0413 - val_loss: 0.1692 - val_mae: 0.0317\n","\n","Epoch 00112: val_loss improved from 0.16920 to 0.16920, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 113/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1888 - mae: 0.0414 - val_loss: 0.1692 - val_mae: 0.0315\n","\n","Epoch 00113: val_loss improved from 0.16920 to 0.16919, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 114/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1890 - mae: 0.0447 - val_loss: 0.1692 - val_mae: 0.0314\n","\n","Epoch 00114: val_loss improved from 0.16919 to 0.16915, saving model to /content/drive/My Drive/segmentation/weightsRegressor/\n","Epoch 115/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1884 - mae: 0.0424 - val_loss: 0.1692 - val_mae: 0.0313\n","\n","Epoch 00115: val_loss did not improve from 0.16915\n","Epoch 116/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1945 - mae: 0.0449 - val_loss: 0.1692 - val_mae: 0.0313\n","\n","Epoch 00116: val_loss did not improve from 0.16915\n","Epoch 117/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1901 - mae: 0.0432 - val_loss: 0.1692 - val_mae: 0.0314\n","\n","Epoch 00117: val_loss did not improve from 0.16915\n","Epoch 118/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1827 - mae: 0.0438 - val_loss: 0.1692 - val_mae: 0.0313\n","\n","Epoch 00118: val_loss did not improve from 0.16915\n","Epoch 119/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1999 - mae: 0.0449 - val_loss: 0.1693 - val_mae: 0.0313\n","\n","Epoch 00119: val_loss did not improve from 0.16915\n","Epoch 120/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1862 - mae: 0.0417 - val_loss: 0.1694 - val_mae: 0.0314\n","\n","Epoch 00120: val_loss did not improve from 0.16915\n","Epoch 121/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1828 - mae: 0.0406 - val_loss: 0.1694 - val_mae: 0.0314\n","\n","Epoch 00121: val_loss did not improve from 0.16915\n","Epoch 122/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1894 - mae: 0.0404 - val_loss: 0.1696 - val_mae: 0.0315\n","\n","Epoch 00122: val_loss did not improve from 0.16915\n","Epoch 123/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1843 - mae: 0.0410 - val_loss: 0.1696 - val_mae: 0.0315\n","\n","Epoch 00123: val_loss did not improve from 0.16915\n","Epoch 124/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1925 - mae: 0.0398 - val_loss: 0.1695 - val_mae: 0.0315\n","\n","Epoch 00124: val_loss did not improve from 0.16915\n","Epoch 125/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1849 - mae: 0.0422 - val_loss: 0.1695 - val_mae: 0.0315\n","\n","Epoch 00125: val_loss did not improve from 0.16915\n","Epoch 126/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1873 - mae: 0.0394 - val_loss: 0.1695 - val_mae: 0.0315\n","\n","Epoch 00126: val_loss did not improve from 0.16915\n","Epoch 127/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1868 - mae: 0.0433 - val_loss: 0.1694 - val_mae: 0.0314\n","\n","Epoch 00127: val_loss did not improve from 0.16915\n","Epoch 128/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1797 - mae: 0.0387 - val_loss: 0.1694 - val_mae: 0.0313\n","\n","Epoch 00128: val_loss did not improve from 0.16915\n","Epoch 129/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1807 - mae: 0.0409 - val_loss: 0.1694 - val_mae: 0.0313\n","\n","Epoch 00129: val_loss did not improve from 0.16915\n","Epoch 130/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1816 - mae: 0.0401 - val_loss: 0.1694 - val_mae: 0.0313\n","\n","Epoch 00130: val_loss did not improve from 0.16915\n","Epoch 131/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1883 - mae: 0.0409 - val_loss: 0.1695 - val_mae: 0.0313\n","\n","Epoch 00131: val_loss did not improve from 0.16915\n","Epoch 132/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1866 - mae: 0.0390 - val_loss: 0.1695 - val_mae: 0.0314\n","\n","Epoch 00132: val_loss did not improve from 0.16915\n","Epoch 133/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1896 - mae: 0.0423 - val_loss: 0.1695 - val_mae: 0.0315\n","\n","Epoch 00133: val_loss did not improve from 0.16915\n","Epoch 134/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1854 - mae: 0.0428 - val_loss: 0.1695 - val_mae: 0.0315\n","\n","Epoch 00134: val_loss did not improve from 0.16915\n","Epoch 135/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1939 - mae: 0.0448 - val_loss: 0.1695 - val_mae: 0.0314\n","\n","Epoch 00135: val_loss did not improve from 0.16915\n","Epoch 136/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1825 - mae: 0.0393 - val_loss: 0.1695 - val_mae: 0.0314\n","\n","Epoch 00136: val_loss did not improve from 0.16915\n","Epoch 137/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1780 - mae: 0.0395 - val_loss: 0.1695 - val_mae: 0.0313\n","\n","Epoch 00137: val_loss did not improve from 0.16915\n","Epoch 138/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1844 - mae: 0.0430 - val_loss: 0.1694 - val_mae: 0.0313\n","\n","Epoch 00138: val_loss did not improve from 0.16915\n","Epoch 139/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1751 - mae: 0.0388 - val_loss: 0.1695 - val_mae: 0.0314\n","\n","Epoch 00139: val_loss did not improve from 0.16915\n","Epoch 140/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1848 - mae: 0.0431 - val_loss: 0.1695 - val_mae: 0.0314\n","\n","Epoch 00140: val_loss did not improve from 0.16915\n","Epoch 141/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1901 - mae: 0.0431 - val_loss: 0.1695 - val_mae: 0.0315\n","\n","Epoch 00141: val_loss did not improve from 0.16915\n","Epoch 142/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1870 - mae: 0.0379 - val_loss: 0.1695 - val_mae: 0.0315\n","\n","Epoch 00142: val_loss did not improve from 0.16915\n","Epoch 143/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1859 - mae: 0.0412 - val_loss: 0.1695 - val_mae: 0.0315\n","\n","Epoch 00143: val_loss did not improve from 0.16915\n","Epoch 144/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1826 - mae: 0.0400 - val_loss: 0.1695 - val_mae: 0.0315\n","\n","Epoch 00144: val_loss did not improve from 0.16915\n","Epoch 145/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1851 - mae: 0.0419 - val_loss: 0.1695 - val_mae: 0.0316\n","\n","Epoch 00145: val_loss did not improve from 0.16915\n","Epoch 146/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1806 - mae: 0.0419 - val_loss: 0.1695 - val_mae: 0.0316\n","\n","Epoch 00146: val_loss did not improve from 0.16915\n","Epoch 147/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1844 - mae: 0.0417 - val_loss: 0.1695 - val_mae: 0.0316\n","\n","Epoch 00147: val_loss did not improve from 0.16915\n","Epoch 148/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1864 - mae: 0.0426 - val_loss: 0.1695 - val_mae: 0.0316\n","\n","Epoch 00148: val_loss did not improve from 0.16915\n","Epoch 149/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1840 - mae: 0.0425 - val_loss: 0.1695 - val_mae: 0.0316\n","\n","Epoch 00149: val_loss did not improve from 0.16915\n","Epoch 150/150\n","6/6 [==============================] - 8s 1s/step - loss: 0.1783 - mae: 0.0417 - val_loss: 0.1695 - val_mae: 0.0316\n","\n","Epoch 00150: val_loss did not improve from 0.16915\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vqGrMEiZhKQ","executionInfo":{"status":"ok","timestamp":1618192675354,"user_tz":-180,"elapsed":17449,"user":{"displayName":"University of Artificial Intelligence","photoUrl":"","userId":"07499422860102443925"}},"outputId":"3e69c8fd-31e1-47eb-82f7-9a2a5d23b13d"},"source":["input_layer = Input((x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n","resize_layer = Resizing(224, 224)(input_layer)\n","\n","resnet = VGG19()(resize_layer)\n","\n","x = Dropout(0.1)(resnet)\n","out = Dense(8, activation = 'sigmoid')(x)\n","\n","model = Model(input_layer, out)\n","\n","model.compile(optimizer=Adam(lr = 0.001), loss='binary_crossentropy', metrics=['mae'])\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, 512, 768, 3)]     0         \n","_________________________________________________________________\n","resizing (Resizing)          (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","vgg19 (Functional)           (None, 1000)              143667240 \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 1000)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 8)                 8008      \n","=================================================================\n","Total params: 143,675,248\n","Trainable params: 143,675,248\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hl5CKkORqN0_"},"source":["# Пробуем Предобученную Сетку"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7SP2a9lZwFI","executionInfo":{"status":"ok","timestamp":1618193289939,"user_tz":-180,"elapsed":630089,"user":{"displayName":"University of Artificial Intelligence","photoUrl":"","userId":"07499422860102443925"}},"outputId":"049c7458-6a19-455f-ce3b-9b25b8a95376"},"source":["from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","\n","cb = ReduceLROnPlateau()\n","\n","history = model.fit(x_train/255, y_train, epochs=250, batch_size=8, validation_data = (x_test/255, y_test), callbacks = [cb]) #  Обучаем модель на выборке по трем классам на полноразмерных изображениях"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/250\n","11/11 [==============================] - 43s 632ms/step - loss: 0.6846 - mae: 0.4547 - val_loss: 0.6749 - val_mae: 0.4483\n","Epoch 2/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.6725 - mae: 0.4484 - val_loss: 0.6651 - val_mae: 0.4428\n","Epoch 3/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.6640 - mae: 0.4386 - val_loss: 0.6555 - val_mae: 0.4374\n","Epoch 4/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.6533 - mae: 0.4326 - val_loss: 0.6461 - val_mae: 0.4320\n","Epoch 5/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.6424 - mae: 0.4335 - val_loss: 0.6369 - val_mae: 0.4266\n","Epoch 6/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.6374 - mae: 0.4241 - val_loss: 0.6279 - val_mae: 0.4214\n","Epoch 7/250\n","11/11 [==============================] - 2s 213ms/step - loss: 0.6277 - mae: 0.4246 - val_loss: 0.6190 - val_mae: 0.4161\n","Epoch 8/250\n","11/11 [==============================] - 2s 214ms/step - loss: 0.6156 - mae: 0.4135 - val_loss: 0.6101 - val_mae: 0.4108\n","Epoch 9/250\n","11/11 [==============================] - 2s 213ms/step - loss: 0.6088 - mae: 0.4062 - val_loss: 0.6014 - val_mae: 0.4055\n","Epoch 10/250\n","11/11 [==============================] - 2s 214ms/step - loss: 0.5963 - mae: 0.4051 - val_loss: 0.5928 - val_mae: 0.4002\n","Epoch 11/250\n","11/11 [==============================] - 2s 213ms/step - loss: 0.5894 - mae: 0.3986 - val_loss: 0.5844 - val_mae: 0.3950\n","Epoch 12/250\n","11/11 [==============================] - 2s 215ms/step - loss: 0.5802 - mae: 0.3930 - val_loss: 0.5763 - val_mae: 0.3899\n","Epoch 13/250\n","11/11 [==============================] - 2s 215ms/step - loss: 0.5727 - mae: 0.3879 - val_loss: 0.5682 - val_mae: 0.3848\n","Epoch 14/250\n","11/11 [==============================] - 2s 214ms/step - loss: 0.5699 - mae: 0.3872 - val_loss: 0.5604 - val_mae: 0.3798\n","Epoch 15/250\n","11/11 [==============================] - 2s 215ms/step - loss: 0.5611 - mae: 0.3810 - val_loss: 0.5529 - val_mae: 0.3750\n","Epoch 16/250\n","11/11 [==============================] - 2s 213ms/step - loss: 0.5583 - mae: 0.3793 - val_loss: 0.5455 - val_mae: 0.3702\n","Epoch 17/250\n","11/11 [==============================] - 2s 212ms/step - loss: 0.5443 - mae: 0.3678 - val_loss: 0.5381 - val_mae: 0.3653\n","Epoch 18/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.5312 - mae: 0.3605 - val_loss: 0.5308 - val_mae: 0.3605\n","Epoch 19/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.5287 - mae: 0.3602 - val_loss: 0.5237 - val_mae: 0.3557\n","Epoch 20/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.5229 - mae: 0.3549 - val_loss: 0.5168 - val_mae: 0.3510\n","Epoch 21/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.5175 - mae: 0.3480 - val_loss: 0.5101 - val_mae: 0.3464\n","Epoch 22/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.5084 - mae: 0.3461 - val_loss: 0.5035 - val_mae: 0.3420\n","Epoch 23/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.5113 - mae: 0.3491 - val_loss: 0.4972 - val_mae: 0.3376\n","Epoch 24/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.4885 - mae: 0.3345 - val_loss: 0.4908 - val_mae: 0.3331\n","Epoch 25/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.4909 - mae: 0.3314 - val_loss: 0.4845 - val_mae: 0.3286\n","Epoch 26/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.4899 - mae: 0.3294 - val_loss: 0.4783 - val_mae: 0.3243\n","Epoch 27/250\n","11/11 [==============================] - 2s 205ms/step - loss: 0.4755 - mae: 0.3255 - val_loss: 0.4724 - val_mae: 0.3200\n","Epoch 28/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.4743 - mae: 0.3215 - val_loss: 0.4665 - val_mae: 0.3157\n","Epoch 29/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.4715 - mae: 0.3217 - val_loss: 0.4608 - val_mae: 0.3116\n","Epoch 30/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.4561 - mae: 0.3077 - val_loss: 0.4551 - val_mae: 0.3074\n","Epoch 31/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.4544 - mae: 0.3049 - val_loss: 0.4495 - val_mae: 0.3032\n","Epoch 32/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.4489 - mae: 0.3047 - val_loss: 0.4441 - val_mae: 0.2992\n","Epoch 33/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.4440 - mae: 0.2973 - val_loss: 0.4388 - val_mae: 0.2952\n","Epoch 34/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.4428 - mae: 0.3003 - val_loss: 0.4336 - val_mae: 0.2912\n","Epoch 35/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.4355 - mae: 0.2921 - val_loss: 0.4285 - val_mae: 0.2873\n","Epoch 36/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.4280 - mae: 0.2851 - val_loss: 0.4235 - val_mae: 0.2835\n","Epoch 37/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.4331 - mae: 0.2872 - val_loss: 0.4187 - val_mae: 0.2797\n","Epoch 38/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.4197 - mae: 0.2821 - val_loss: 0.4139 - val_mae: 0.2760\n","Epoch 39/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.4142 - mae: 0.2711 - val_loss: 0.4093 - val_mae: 0.2723\n","Epoch 40/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.4143 - mae: 0.2764 - val_loss: 0.4047 - val_mae: 0.2687\n","Epoch 41/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.4040 - mae: 0.2647 - val_loss: 0.4003 - val_mae: 0.2652\n","Epoch 42/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.4050 - mae: 0.2647 - val_loss: 0.3959 - val_mae: 0.2616\n","Epoch 43/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.4023 - mae: 0.2645 - val_loss: 0.3916 - val_mae: 0.2582\n","Epoch 44/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.4022 - mae: 0.2673 - val_loss: 0.3874 - val_mae: 0.2548\n","Epoch 45/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3926 - mae: 0.2561 - val_loss: 0.3833 - val_mae: 0.2514\n","Epoch 46/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.3714 - mae: 0.2441 - val_loss: 0.3793 - val_mae: 0.2481\n","Epoch 47/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.3766 - mae: 0.2452 - val_loss: 0.3753 - val_mae: 0.2447\n","Epoch 48/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3721 - mae: 0.2487 - val_loss: 0.3714 - val_mae: 0.2415\n","Epoch 49/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3858 - mae: 0.2519 - val_loss: 0.3676 - val_mae: 0.2383\n","Epoch 50/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3806 - mae: 0.2462 - val_loss: 0.3639 - val_mae: 0.2352\n","Epoch 51/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3551 - mae: 0.2298 - val_loss: 0.3602 - val_mae: 0.2320\n","Epoch 52/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3574 - mae: 0.2357 - val_loss: 0.3566 - val_mae: 0.2289\n","Epoch 53/250\n","11/11 [==============================] - 2s 212ms/step - loss: 0.3613 - mae: 0.2326 - val_loss: 0.3532 - val_mae: 0.2259\n","Epoch 54/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.3586 - mae: 0.2341 - val_loss: 0.3497 - val_mae: 0.2230\n","Epoch 55/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.3575 - mae: 0.2245 - val_loss: 0.3464 - val_mae: 0.2200\n","Epoch 56/250\n","11/11 [==============================] - 2s 212ms/step - loss: 0.3528 - mae: 0.2212 - val_loss: 0.3431 - val_mae: 0.2171\n","Epoch 57/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.3394 - mae: 0.2132 - val_loss: 0.3399 - val_mae: 0.2143\n","Epoch 58/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.3407 - mae: 0.2100 - val_loss: 0.3368 - val_mae: 0.2115\n","Epoch 59/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.3462 - mae: 0.2164 - val_loss: 0.3337 - val_mae: 0.2087\n","Epoch 60/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.3334 - mae: 0.2082 - val_loss: 0.3306 - val_mae: 0.2060\n","Epoch 61/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3434 - mae: 0.2099 - val_loss: 0.3277 - val_mae: 0.2033\n","Epoch 62/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.3272 - mae: 0.2070 - val_loss: 0.3247 - val_mae: 0.2006\n","Epoch 63/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.3230 - mae: 0.1951 - val_loss: 0.3219 - val_mae: 0.1980\n","Epoch 64/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.3185 - mae: 0.1981 - val_loss: 0.3191 - val_mae: 0.1954\n","Epoch 65/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3227 - mae: 0.1967 - val_loss: 0.3163 - val_mae: 0.1929\n","Epoch 66/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.3141 - mae: 0.1868 - val_loss: 0.3137 - val_mae: 0.1904\n","Epoch 67/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.3159 - mae: 0.1938 - val_loss: 0.3110 - val_mae: 0.1879\n","Epoch 68/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.3173 - mae: 0.1892 - val_loss: 0.3084 - val_mae: 0.1855\n","Epoch 69/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2994 - mae: 0.1775 - val_loss: 0.3059 - val_mae: 0.1831\n","Epoch 70/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3010 - mae: 0.1740 - val_loss: 0.3034 - val_mae: 0.1807\n","Epoch 71/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.3096 - mae: 0.1904 - val_loss: 0.3010 - val_mae: 0.1783\n","Epoch 72/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3075 - mae: 0.1872 - val_loss: 0.2986 - val_mae: 0.1760\n","Epoch 73/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.3010 - mae: 0.1792 - val_loss: 0.2963 - val_mae: 0.1738\n","Epoch 74/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2896 - mae: 0.1724 - val_loss: 0.2940 - val_mae: 0.1716\n","Epoch 75/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2956 - mae: 0.1762 - val_loss: 0.2917 - val_mae: 0.1694\n","Epoch 76/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2866 - mae: 0.1636 - val_loss: 0.2896 - val_mae: 0.1672\n","Epoch 77/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.3014 - mae: 0.1745 - val_loss: 0.2874 - val_mae: 0.1651\n","Epoch 78/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2843 - mae: 0.1647 - val_loss: 0.2853 - val_mae: 0.1630\n","Epoch 79/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2857 - mae: 0.1599 - val_loss: 0.2832 - val_mae: 0.1609\n","Epoch 80/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2854 - mae: 0.1617 - val_loss: 0.2812 - val_mae: 0.1589\n","Epoch 81/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2878 - mae: 0.1627 - val_loss: 0.2792 - val_mae: 0.1568\n","Epoch 82/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2873 - mae: 0.1624 - val_loss: 0.2772 - val_mae: 0.1548\n","Epoch 83/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2939 - mae: 0.1624 - val_loss: 0.2753 - val_mae: 0.1529\n","Epoch 84/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2797 - mae: 0.1551 - val_loss: 0.2734 - val_mae: 0.1510\n","Epoch 85/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2844 - mae: 0.1603 - val_loss: 0.2716 - val_mae: 0.1491\n","Epoch 86/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2741 - mae: 0.1577 - val_loss: 0.2698 - val_mae: 0.1472\n","Epoch 87/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.2849 - mae: 0.1614 - val_loss: 0.2681 - val_mae: 0.1454\n","Epoch 88/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2844 - mae: 0.1579 - val_loss: 0.2663 - val_mae: 0.1436\n","Epoch 89/250\n","11/11 [==============================] - 2s 212ms/step - loss: 0.2723 - mae: 0.1549 - val_loss: 0.2646 - val_mae: 0.1418\n","Epoch 90/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2780 - mae: 0.1445 - val_loss: 0.2630 - val_mae: 0.1400\n","Epoch 91/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2722 - mae: 0.1430 - val_loss: 0.2613 - val_mae: 0.1383\n","Epoch 92/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2731 - mae: 0.1412 - val_loss: 0.2597 - val_mae: 0.1365\n","Epoch 93/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2612 - mae: 0.1368 - val_loss: 0.2581 - val_mae: 0.1348\n","Epoch 94/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2678 - mae: 0.1466 - val_loss: 0.2565 - val_mae: 0.1331\n","Epoch 95/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2480 - mae: 0.1315 - val_loss: 0.2550 - val_mae: 0.1315\n","Epoch 96/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2591 - mae: 0.1327 - val_loss: 0.2535 - val_mae: 0.1299\n","Epoch 97/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2581 - mae: 0.1384 - val_loss: 0.2521 - val_mae: 0.1283\n","Epoch 98/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2572 - mae: 0.1367 - val_loss: 0.2506 - val_mae: 0.1267\n","Epoch 99/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2514 - mae: 0.1289 - val_loss: 0.2492 - val_mae: 0.1251\n","Epoch 100/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2625 - mae: 0.1394 - val_loss: 0.2478 - val_mae: 0.1236\n","Epoch 101/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2450 - mae: 0.1280 - val_loss: 0.2465 - val_mae: 0.1220\n","Epoch 102/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2417 - mae: 0.1243 - val_loss: 0.2451 - val_mae: 0.1206\n","Epoch 103/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2517 - mae: 0.1302 - val_loss: 0.2439 - val_mae: 0.1191\n","Epoch 104/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2448 - mae: 0.1254 - val_loss: 0.2426 - val_mae: 0.1177\n","Epoch 105/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2568 - mae: 0.1306 - val_loss: 0.2413 - val_mae: 0.1163\n","Epoch 106/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2358 - mae: 0.1165 - val_loss: 0.2401 - val_mae: 0.1149\n","Epoch 107/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2408 - mae: 0.1155 - val_loss: 0.2389 - val_mae: 0.1135\n","Epoch 108/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2376 - mae: 0.1142 - val_loss: 0.2377 - val_mae: 0.1122\n","Epoch 109/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2299 - mae: 0.1131 - val_loss: 0.2366 - val_mae: 0.1108\n","Epoch 110/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2430 - mae: 0.1125 - val_loss: 0.2354 - val_mae: 0.1095\n","Epoch 111/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2314 - mae: 0.1106 - val_loss: 0.2343 - val_mae: 0.1082\n","Epoch 112/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2318 - mae: 0.1075 - val_loss: 0.2332 - val_mae: 0.1069\n","Epoch 113/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2528 - mae: 0.1300 - val_loss: 0.2321 - val_mae: 0.1056\n","Epoch 114/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.2407 - mae: 0.1099 - val_loss: 0.2311 - val_mae: 0.1044\n","Epoch 115/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2300 - mae: 0.1049 - val_loss: 0.2300 - val_mae: 0.1031\n","Epoch 116/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2245 - mae: 0.0992 - val_loss: 0.2290 - val_mae: 0.1019\n","Epoch 117/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2330 - mae: 0.0991 - val_loss: 0.2280 - val_mae: 0.1008\n","Epoch 118/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2422 - mae: 0.1161 - val_loss: 0.2270 - val_mae: 0.0996\n","Epoch 119/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2354 - mae: 0.1075 - val_loss: 0.2260 - val_mae: 0.0985\n","Epoch 120/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2282 - mae: 0.1028 - val_loss: 0.2251 - val_mae: 0.0974\n","Epoch 121/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2351 - mae: 0.1147 - val_loss: 0.2242 - val_mae: 0.0962\n","Epoch 122/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2361 - mae: 0.1046 - val_loss: 0.2232 - val_mae: 0.0951\n","Epoch 123/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2372 - mae: 0.1082 - val_loss: 0.2223 - val_mae: 0.0941\n","Epoch 124/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2215 - mae: 0.1020 - val_loss: 0.2214 - val_mae: 0.0930\n","Epoch 125/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2318 - mae: 0.0990 - val_loss: 0.2206 - val_mae: 0.0920\n","Epoch 126/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2284 - mae: 0.1005 - val_loss: 0.2197 - val_mae: 0.0909\n","Epoch 127/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2243 - mae: 0.1037 - val_loss: 0.2189 - val_mae: 0.0899\n","Epoch 128/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2273 - mae: 0.1023 - val_loss: 0.2180 - val_mae: 0.0889\n","Epoch 129/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2203 - mae: 0.0935 - val_loss: 0.2172 - val_mae: 0.0879\n","Epoch 130/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.2168 - mae: 0.0968 - val_loss: 0.2164 - val_mae: 0.0870\n","Epoch 131/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2178 - mae: 0.0960 - val_loss: 0.2157 - val_mae: 0.0860\n","Epoch 132/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.2204 - mae: 0.0947 - val_loss: 0.2149 - val_mae: 0.0851\n","Epoch 133/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2200 - mae: 0.0886 - val_loss: 0.2142 - val_mae: 0.0842\n","Epoch 134/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2253 - mae: 0.1014 - val_loss: 0.2134 - val_mae: 0.0833\n","Epoch 135/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.2260 - mae: 0.0960 - val_loss: 0.2127 - val_mae: 0.0824\n","Epoch 136/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2165 - mae: 0.0917 - val_loss: 0.2120 - val_mae: 0.0815\n","Epoch 137/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.2083 - mae: 0.0874 - val_loss: 0.2113 - val_mae: 0.0806\n","Epoch 138/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2164 - mae: 0.0875 - val_loss: 0.2106 - val_mae: 0.0798\n","Epoch 139/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2206 - mae: 0.0957 - val_loss: 0.2099 - val_mae: 0.0790\n","Epoch 140/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2118 - mae: 0.0876 - val_loss: 0.2092 - val_mae: 0.0782\n","Epoch 141/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2250 - mae: 0.0952 - val_loss: 0.2086 - val_mae: 0.0774\n","Epoch 142/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2133 - mae: 0.0788 - val_loss: 0.2079 - val_mae: 0.0766\n","Epoch 143/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2165 - mae: 0.0878 - val_loss: 0.2073 - val_mae: 0.0759\n","Epoch 144/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2083 - mae: 0.0825 - val_loss: 0.2067 - val_mae: 0.0751\n","Epoch 145/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2149 - mae: 0.0903 - val_loss: 0.2061 - val_mae: 0.0744\n","Epoch 146/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2134 - mae: 0.0801 - val_loss: 0.2055 - val_mae: 0.0737\n","Epoch 147/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2009 - mae: 0.0746 - val_loss: 0.2049 - val_mae: 0.0730\n","Epoch 148/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.2022 - mae: 0.0827 - val_loss: 0.2043 - val_mae: 0.0724\n","Epoch 149/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1985 - mae: 0.0815 - val_loss: 0.2038 - val_mae: 0.0717\n","Epoch 150/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2069 - mae: 0.0723 - val_loss: 0.2033 - val_mae: 0.0711\n","Epoch 151/250\n","11/11 [==============================] - 2s 212ms/step - loss: 0.2094 - mae: 0.0790 - val_loss: 0.2027 - val_mae: 0.0705\n","Epoch 152/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2230 - mae: 0.0906 - val_loss: 0.2022 - val_mae: 0.0698\n","Epoch 153/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2217 - mae: 0.0890 - val_loss: 0.2017 - val_mae: 0.0692\n","Epoch 154/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2064 - mae: 0.0746 - val_loss: 0.2011 - val_mae: 0.0686\n","Epoch 155/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2017 - mae: 0.0749 - val_loss: 0.2006 - val_mae: 0.0679\n","Epoch 156/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2130 - mae: 0.0820 - val_loss: 0.2001 - val_mae: 0.0673\n","Epoch 157/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2115 - mae: 0.0744 - val_loss: 0.1996 - val_mae: 0.0668\n","Epoch 158/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2025 - mae: 0.0703 - val_loss: 0.1992 - val_mae: 0.0662\n","Epoch 159/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1975 - mae: 0.0734 - val_loss: 0.1987 - val_mae: 0.0656\n","Epoch 160/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2086 - mae: 0.0695 - val_loss: 0.1982 - val_mae: 0.0651\n","Epoch 161/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.2132 - mae: 0.0725 - val_loss: 0.1978 - val_mae: 0.0645\n","Epoch 162/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2072 - mae: 0.0746 - val_loss: 0.1973 - val_mae: 0.0639\n","Epoch 163/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1953 - mae: 0.0614 - val_loss: 0.1969 - val_mae: 0.0634\n","Epoch 164/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2075 - mae: 0.0773 - val_loss: 0.1965 - val_mae: 0.0629\n","Epoch 165/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2073 - mae: 0.0657 - val_loss: 0.1961 - val_mae: 0.0624\n","Epoch 166/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.2047 - mae: 0.0725 - val_loss: 0.1956 - val_mae: 0.0619\n","Epoch 167/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1905 - mae: 0.0601 - val_loss: 0.1952 - val_mae: 0.0614\n","Epoch 168/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1995 - mae: 0.0630 - val_loss: 0.1948 - val_mae: 0.0609\n","Epoch 169/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2162 - mae: 0.0802 - val_loss: 0.1944 - val_mae: 0.0604\n","Epoch 170/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1949 - mae: 0.0662 - val_loss: 0.1940 - val_mae: 0.0599\n","Epoch 171/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1966 - mae: 0.0676 - val_loss: 0.1936 - val_mae: 0.0594\n","Epoch 172/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1961 - mae: 0.0610 - val_loss: 0.1933 - val_mae: 0.0590\n","Epoch 173/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2027 - mae: 0.0733 - val_loss: 0.1929 - val_mae: 0.0586\n","Epoch 174/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2021 - mae: 0.0691 - val_loss: 0.1926 - val_mae: 0.0581\n","Epoch 175/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1963 - mae: 0.0649 - val_loss: 0.1922 - val_mae: 0.0577\n","Epoch 176/250\n","11/11 [==============================] - 2s 212ms/step - loss: 0.1926 - mae: 0.0650 - val_loss: 0.1919 - val_mae: 0.0573\n","Epoch 177/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2005 - mae: 0.0669 - val_loss: 0.1915 - val_mae: 0.0569\n","Epoch 178/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1994 - mae: 0.0702 - val_loss: 0.1912 - val_mae: 0.0565\n","Epoch 179/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2039 - mae: 0.0649 - val_loss: 0.1908 - val_mae: 0.0561\n","Epoch 180/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1912 - mae: 0.0628 - val_loss: 0.1905 - val_mae: 0.0557\n","Epoch 181/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2063 - mae: 0.0690 - val_loss: 0.1902 - val_mae: 0.0553\n","Epoch 182/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1927 - mae: 0.0627 - val_loss: 0.1899 - val_mae: 0.0550\n","Epoch 183/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1970 - mae: 0.0694 - val_loss: 0.1896 - val_mae: 0.0546\n","Epoch 184/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1970 - mae: 0.0659 - val_loss: 0.1893 - val_mae: 0.0542\n","Epoch 185/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2109 - mae: 0.0713 - val_loss: 0.1890 - val_mae: 0.0539\n","Epoch 186/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1979 - mae: 0.0619 - val_loss: 0.1887 - val_mae: 0.0535\n","Epoch 187/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2034 - mae: 0.0658 - val_loss: 0.1884 - val_mae: 0.0531\n","Epoch 188/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1985 - mae: 0.0637 - val_loss: 0.1881 - val_mae: 0.0528\n","Epoch 189/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2074 - mae: 0.0684 - val_loss: 0.1878 - val_mae: 0.0525\n","Epoch 190/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1933 - mae: 0.0610 - val_loss: 0.1876 - val_mae: 0.0521\n","Epoch 191/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1849 - mae: 0.0561 - val_loss: 0.1873 - val_mae: 0.0518\n","Epoch 192/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1955 - mae: 0.0554 - val_loss: 0.1870 - val_mae: 0.0515\n","Epoch 193/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1989 - mae: 0.0672 - val_loss: 0.1868 - val_mae: 0.0512\n","Epoch 194/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1914 - mae: 0.0649 - val_loss: 0.1865 - val_mae: 0.0508\n","Epoch 195/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2018 - mae: 0.0644 - val_loss: 0.1863 - val_mae: 0.0505\n","Epoch 196/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1990 - mae: 0.0622 - val_loss: 0.1860 - val_mae: 0.0502\n","Epoch 197/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1802 - mae: 0.0521 - val_loss: 0.1858 - val_mae: 0.0499\n","Epoch 198/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1996 - mae: 0.0670 - val_loss: 0.1855 - val_mae: 0.0497\n","Epoch 199/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.2035 - mae: 0.0662 - val_loss: 0.1853 - val_mae: 0.0494\n","Epoch 200/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1965 - mae: 0.0599 - val_loss: 0.1850 - val_mae: 0.0491\n","Epoch 201/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1995 - mae: 0.0625 - val_loss: 0.1848 - val_mae: 0.0488\n","Epoch 202/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1930 - mae: 0.0546 - val_loss: 0.1846 - val_mae: 0.0486\n","Epoch 203/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1988 - mae: 0.0592 - val_loss: 0.1843 - val_mae: 0.0483\n","Epoch 204/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1873 - mae: 0.0515 - val_loss: 0.1842 - val_mae: 0.0481\n","Epoch 205/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1999 - mae: 0.0586 - val_loss: 0.1840 - val_mae: 0.0478\n","Epoch 206/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1840 - mae: 0.0515 - val_loss: 0.1838 - val_mae: 0.0476\n","Epoch 207/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1852 - mae: 0.0536 - val_loss: 0.1836 - val_mae: 0.0474\n","Epoch 208/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1880 - mae: 0.0529 - val_loss: 0.1834 - val_mae: 0.0472\n","Epoch 209/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1932 - mae: 0.0533 - val_loss: 0.1832 - val_mae: 0.0470\n","Epoch 210/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1906 - mae: 0.0560 - val_loss: 0.1830 - val_mae: 0.0467\n","Epoch 211/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1913 - mae: 0.0596 - val_loss: 0.1828 - val_mae: 0.0465\n","Epoch 212/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1872 - mae: 0.0514 - val_loss: 0.1826 - val_mae: 0.0463\n","Epoch 213/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1958 - mae: 0.0639 - val_loss: 0.1824 - val_mae: 0.0460\n","Epoch 214/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1837 - mae: 0.0531 - val_loss: 0.1823 - val_mae: 0.0458\n","Epoch 215/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1902 - mae: 0.0533 - val_loss: 0.1821 - val_mae: 0.0456\n","Epoch 216/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1835 - mae: 0.0459 - val_loss: 0.1819 - val_mae: 0.0454\n","Epoch 217/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1951 - mae: 0.0543 - val_loss: 0.1818 - val_mae: 0.0452\n","Epoch 218/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1843 - mae: 0.0519 - val_loss: 0.1816 - val_mae: 0.0450\n","Epoch 219/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.2092 - mae: 0.0622 - val_loss: 0.1814 - val_mae: 0.0448\n","Epoch 220/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1911 - mae: 0.0518 - val_loss: 0.1813 - val_mae: 0.0446\n","Epoch 221/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1911 - mae: 0.0549 - val_loss: 0.1811 - val_mae: 0.0444\n","Epoch 222/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1885 - mae: 0.0459 - val_loss: 0.1810 - val_mae: 0.0442\n","Epoch 223/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1828 - mae: 0.0534 - val_loss: 0.1808 - val_mae: 0.0440\n","Epoch 224/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1956 - mae: 0.0515 - val_loss: 0.1807 - val_mae: 0.0439\n","Epoch 225/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1801 - mae: 0.0461 - val_loss: 0.1806 - val_mae: 0.0437\n","Epoch 226/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1806 - mae: 0.0421 - val_loss: 0.1805 - val_mae: 0.0436\n","Epoch 227/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1891 - mae: 0.0572 - val_loss: 0.1803 - val_mae: 0.0434\n","Epoch 228/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1970 - mae: 0.0613 - val_loss: 0.1802 - val_mae: 0.0432\n","Epoch 229/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1940 - mae: 0.0538 - val_loss: 0.1800 - val_mae: 0.0430\n","Epoch 230/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1979 - mae: 0.0580 - val_loss: 0.1799 - val_mae: 0.0428\n","Epoch 231/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1991 - mae: 0.0514 - val_loss: 0.1798 - val_mae: 0.0427\n","Epoch 232/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1835 - mae: 0.0485 - val_loss: 0.1796 - val_mae: 0.0425\n","Epoch 233/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1879 - mae: 0.0529 - val_loss: 0.1795 - val_mae: 0.0423\n","Epoch 234/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1787 - mae: 0.0435 - val_loss: 0.1794 - val_mae: 0.0422\n","Epoch 235/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1921 - mae: 0.0465 - val_loss: 0.1793 - val_mae: 0.0420\n","Epoch 236/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1780 - mae: 0.0482 - val_loss: 0.1791 - val_mae: 0.0418\n","Epoch 237/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1792 - mae: 0.0512 - val_loss: 0.1790 - val_mae: 0.0417\n","Epoch 238/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.2025 - mae: 0.0607 - val_loss: 0.1789 - val_mae: 0.0415\n","Epoch 239/250\n","11/11 [==============================] - 2s 212ms/step - loss: 0.1858 - mae: 0.0527 - val_loss: 0.1788 - val_mae: 0.0414\n","Epoch 240/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1776 - mae: 0.0395 - val_loss: 0.1787 - val_mae: 0.0413\n","Epoch 241/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1834 - mae: 0.0467 - val_loss: 0.1786 - val_mae: 0.0412\n","Epoch 242/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1870 - mae: 0.0496 - val_loss: 0.1785 - val_mae: 0.0410\n","Epoch 243/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1873 - mae: 0.0464 - val_loss: 0.1784 - val_mae: 0.0409\n","Epoch 244/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1973 - mae: 0.0548 - val_loss: 0.1783 - val_mae: 0.0408\n","Epoch 245/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1870 - mae: 0.0574 - val_loss: 0.1782 - val_mae: 0.0406\n","Epoch 246/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1877 - mae: 0.0502 - val_loss: 0.1781 - val_mae: 0.0405\n","Epoch 247/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1834 - mae: 0.0428 - val_loss: 0.1780 - val_mae: 0.0404\n","Epoch 248/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1834 - mae: 0.0525 - val_loss: 0.1779 - val_mae: 0.0403\n","Epoch 249/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1908 - mae: 0.0486 - val_loss: 0.1778 - val_mae: 0.0401\n","Epoch 250/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1758 - mae: 0.0470 - val_loss: 0.1777 - val_mae: 0.0400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmp2r8L_0-pK","executionInfo":{"status":"ok","timestamp":1618193874499,"user_tz":-180,"elapsed":572453,"user":{"displayName":"University of Artificial Intelligence","photoUrl":"","userId":"07499422860102443925"}},"outputId":"231777b9-714d-48ee-b3d0-0e836cb3061d"},"source":["history = model.fit(x_train/255, y_train, epochs=250, batch_size=8, validation_data = (x_test/255, y_test), callbacks = [cb]) #  Обучаем модель на выборке по трем классам на полноразмерных изображениях"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/250\n","11/11 [==============================] - 2s 212ms/step - loss: 0.1891 - mae: 0.0528 - val_loss: 0.1776 - val_mae: 0.0399\n","Epoch 2/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1894 - mae: 0.0530 - val_loss: 0.1775 - val_mae: 0.0398\n","Epoch 3/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1842 - mae: 0.0478 - val_loss: 0.1774 - val_mae: 0.0397\n","Epoch 4/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1858 - mae: 0.0488 - val_loss: 0.1774 - val_mae: 0.0395\n","Epoch 5/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1831 - mae: 0.0453 - val_loss: 0.1773 - val_mae: 0.0394\n","Epoch 6/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1824 - mae: 0.0441 - val_loss: 0.1772 - val_mae: 0.0394\n","Epoch 7/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1854 - mae: 0.0486 - val_loss: 0.1771 - val_mae: 0.0393\n","Epoch 8/250\n","11/11 [==============================] - 2s 212ms/step - loss: 0.1824 - mae: 0.0444 - val_loss: 0.1771 - val_mae: 0.0391\n","Epoch 9/250\n","11/11 [==============================] - 2s 217ms/step - loss: 0.1842 - mae: 0.0488 - val_loss: 0.1770 - val_mae: 0.0390\n","Epoch 10/250\n","11/11 [==============================] - 2s 214ms/step - loss: 0.1830 - mae: 0.0453 - val_loss: 0.1769 - val_mae: 0.0389\n","Epoch 11/250\n","11/11 [==============================] - 2s 215ms/step - loss: 0.1825 - mae: 0.0452 - val_loss: 0.1768 - val_mae: 0.0388\n","Epoch 12/250\n","11/11 [==============================] - 2s 214ms/step - loss: 0.1877 - mae: 0.0514 - val_loss: 0.1767 - val_mae: 0.0387\n","Epoch 13/250\n","11/11 [==============================] - 2s 213ms/step - loss: 0.1891 - mae: 0.0537 - val_loss: 0.1767 - val_mae: 0.0386\n","Epoch 14/250\n","11/11 [==============================] - 2s 214ms/step - loss: 0.1779 - mae: 0.0395 - val_loss: 0.1766 - val_mae: 0.0385\n","Epoch 15/250\n","11/11 [==============================] - 2s 213ms/step - loss: 0.1836 - mae: 0.0462 - val_loss: 0.1766 - val_mae: 0.0385\n","Epoch 16/250\n","11/11 [==============================] - 2s 211ms/step - loss: 0.1820 - mae: 0.0451 - val_loss: 0.1765 - val_mae: 0.0384\n","Epoch 17/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1804 - mae: 0.0424 - val_loss: 0.1764 - val_mae: 0.0383\n","Epoch 18/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1843 - mae: 0.0463 - val_loss: 0.1764 - val_mae: 0.0382\n","Epoch 19/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1835 - mae: 0.0455 - val_loss: 0.1763 - val_mae: 0.0381\n","Epoch 20/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1783 - mae: 0.0395 - val_loss: 0.1763 - val_mae: 0.0381\n","Epoch 21/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1792 - mae: 0.0412 - val_loss: 0.1762 - val_mae: 0.0380\n","Epoch 22/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1852 - mae: 0.0481 - val_loss: 0.1762 - val_mae: 0.0379\n","Epoch 23/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1849 - mae: 0.0471 - val_loss: 0.1761 - val_mae: 0.0378\n","Epoch 24/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1818 - mae: 0.0443 - val_loss: 0.1760 - val_mae: 0.0377\n","Epoch 25/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1847 - mae: 0.0459 - val_loss: 0.1760 - val_mae: 0.0377\n","Epoch 26/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1790 - mae: 0.0408 - val_loss: 0.1759 - val_mae: 0.0376\n","Epoch 27/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1781 - mae: 0.0393 - val_loss: 0.1759 - val_mae: 0.0375\n","Epoch 28/250\n","11/11 [==============================] - 2s 205ms/step - loss: 0.1823 - mae: 0.0439 - val_loss: 0.1759 - val_mae: 0.0375\n","Epoch 29/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1770 - mae: 0.0379 - val_loss: 0.1758 - val_mae: 0.0374\n","Epoch 30/250\n","11/11 [==============================] - 2s 205ms/step - loss: 0.1797 - mae: 0.0424 - val_loss: 0.1758 - val_mae: 0.0373\n","Epoch 31/250\n","11/11 [==============================] - 2s 205ms/step - loss: 0.1803 - mae: 0.0411 - val_loss: 0.1757 - val_mae: 0.0373\n","Epoch 32/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.1806 - mae: 0.0419 - val_loss: 0.1757 - val_mae: 0.0372\n","Epoch 33/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1824 - mae: 0.0436 - val_loss: 0.1757 - val_mae: 0.0372\n","Epoch 34/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.1822 - mae: 0.0431 - val_loss: 0.1756 - val_mae: 0.0371\n","Epoch 35/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1825 - mae: 0.0435 - val_loss: 0.1756 - val_mae: 0.0370\n","Epoch 36/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1798 - mae: 0.0412 - val_loss: 0.1755 - val_mae: 0.0370\n","Epoch 37/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1818 - mae: 0.0435 - val_loss: 0.1755 - val_mae: 0.0369\n","Epoch 38/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1842 - mae: 0.0454 - val_loss: 0.1755 - val_mae: 0.0369\n","Epoch 39/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.1815 - mae: 0.0430 - val_loss: 0.1754 - val_mae: 0.0368\n","Epoch 40/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1857 - mae: 0.0473 - val_loss: 0.1754 - val_mae: 0.0367\n","Epoch 41/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1784 - mae: 0.0396 - val_loss: 0.1754 - val_mae: 0.0367\n","Epoch 42/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1813 - mae: 0.0435 - val_loss: 0.1753 - val_mae: 0.0366\n","Epoch 43/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.1848 - mae: 0.0478 - val_loss: 0.1753 - val_mae: 0.0366\n","Epoch 44/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1841 - mae: 0.0460 - val_loss: 0.1753 - val_mae: 0.0365\n","Epoch 45/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1785 - mae: 0.0393 - val_loss: 0.1752 - val_mae: 0.0364\n","Epoch 46/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1793 - mae: 0.0397 - val_loss: 0.1752 - val_mae: 0.0364\n","Epoch 47/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1851 - mae: 0.0485 - val_loss: 0.1752 - val_mae: 0.0363\n","Epoch 48/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1840 - mae: 0.0464 - val_loss: 0.1751 - val_mae: 0.0363\n","Epoch 49/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1848 - mae: 0.0473 - val_loss: 0.1751 - val_mae: 0.0362\n","Epoch 50/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1824 - mae: 0.0430 - val_loss: 0.1751 - val_mae: 0.0361\n","Epoch 51/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1793 - mae: 0.0401 - val_loss: 0.1750 - val_mae: 0.0361\n","Epoch 52/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1761 - mae: 0.0361 - val_loss: 0.1750 - val_mae: 0.0361\n","Epoch 53/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1799 - mae: 0.0414 - val_loss: 0.1750 - val_mae: 0.0360\n","Epoch 54/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1801 - mae: 0.0400 - val_loss: 0.1750 - val_mae: 0.0360\n","Epoch 55/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1795 - mae: 0.0394 - val_loss: 0.1750 - val_mae: 0.0359\n","Epoch 56/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1818 - mae: 0.0426 - val_loss: 0.1750 - val_mae: 0.0359\n","Epoch 57/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1788 - mae: 0.0391 - val_loss: 0.1750 - val_mae: 0.0359\n","Epoch 58/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1782 - mae: 0.0404 - val_loss: 0.1749 - val_mae: 0.0358\n","Epoch 59/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1785 - mae: 0.0391 - val_loss: 0.1749 - val_mae: 0.0358\n","Epoch 60/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1804 - mae: 0.0412 - val_loss: 0.1749 - val_mae: 0.0358\n","Epoch 61/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1867 - mae: 0.0495 - val_loss: 0.1749 - val_mae: 0.0357\n","Epoch 62/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1799 - mae: 0.0407 - val_loss: 0.1748 - val_mae: 0.0357\n","Epoch 63/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1776 - mae: 0.0389 - val_loss: 0.1748 - val_mae: 0.0356\n","Epoch 64/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1856 - mae: 0.0476 - val_loss: 0.1748 - val_mae: 0.0356\n","Epoch 65/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1918 - mae: 0.0549 - val_loss: 0.1748 - val_mae: 0.0355\n","Epoch 66/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1760 - mae: 0.0367 - val_loss: 0.1747 - val_mae: 0.0354\n","Epoch 67/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1827 - mae: 0.0438 - val_loss: 0.1747 - val_mae: 0.0354\n","Epoch 68/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1864 - mae: 0.0474 - val_loss: 0.1747 - val_mae: 0.0353\n","Epoch 69/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1795 - mae: 0.0394 - val_loss: 0.1747 - val_mae: 0.0353\n","Epoch 70/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1804 - mae: 0.0417 - val_loss: 0.1746 - val_mae: 0.0353\n","Epoch 71/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1811 - mae: 0.0426 - val_loss: 0.1746 - val_mae: 0.0352\n","Epoch 72/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1811 - mae: 0.0415 - val_loss: 0.1746 - val_mae: 0.0352\n","Epoch 73/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.1802 - mae: 0.0405 - val_loss: 0.1746 - val_mae: 0.0352\n","Epoch 74/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1807 - mae: 0.0421 - val_loss: 0.1746 - val_mae: 0.0352\n","Epoch 75/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1803 - mae: 0.0406 - val_loss: 0.1746 - val_mae: 0.0351\n","Epoch 76/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1814 - mae: 0.0421 - val_loss: 0.1746 - val_mae: 0.0351\n","Epoch 77/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1765 - mae: 0.0360 - val_loss: 0.1746 - val_mae: 0.0351\n","Epoch 78/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1832 - mae: 0.0455 - val_loss: 0.1746 - val_mae: 0.0351\n","Epoch 79/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.1837 - mae: 0.0449 - val_loss: 0.1746 - val_mae: 0.0350\n","Epoch 80/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1799 - mae: 0.0402 - val_loss: 0.1745 - val_mae: 0.0350\n","Epoch 81/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1829 - mae: 0.0434 - val_loss: 0.1745 - val_mae: 0.0350\n","Epoch 82/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1842 - mae: 0.0462 - val_loss: 0.1745 - val_mae: 0.0349\n","Epoch 83/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1771 - mae: 0.0377 - val_loss: 0.1745 - val_mae: 0.0349\n","Epoch 84/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1813 - mae: 0.0417 - val_loss: 0.1745 - val_mae: 0.0349\n","Epoch 85/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1802 - mae: 0.0399 - val_loss: 0.1745 - val_mae: 0.0349\n","Epoch 86/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1812 - mae: 0.0432 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 87/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1771 - mae: 0.0375 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 88/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1768 - mae: 0.0364 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 89/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.1810 - mae: 0.0439 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 90/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1797 - mae: 0.0394 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 91/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1762 - mae: 0.0360 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 92/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1772 - mae: 0.0370 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 93/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1873 - mae: 0.0496 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 94/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1753 - mae: 0.0367 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 95/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1817 - mae: 0.0419 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 96/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1810 - mae: 0.0428 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 97/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1805 - mae: 0.0417 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 98/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1771 - mae: 0.0384 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 99/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1800 - mae: 0.0394 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 100/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1777 - mae: 0.0372 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 101/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1844 - mae: 0.0453 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 102/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1810 - mae: 0.0420 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 103/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1814 - mae: 0.0411 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 104/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1777 - mae: 0.0377 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 105/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1785 - mae: 0.0383 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 106/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1776 - mae: 0.0376 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 107/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1873 - mae: 0.0485 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 108/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1817 - mae: 0.0425 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 109/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1807 - mae: 0.0408 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 110/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1807 - mae: 0.0409 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 111/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1739 - mae: 0.0332 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 112/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1818 - mae: 0.0418 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 113/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1820 - mae: 0.0426 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 114/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1825 - mae: 0.0435 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 115/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1798 - mae: 0.0392 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 116/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1841 - mae: 0.0447 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 117/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1855 - mae: 0.0467 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 118/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1790 - mae: 0.0386 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 119/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1795 - mae: 0.0401 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 120/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1774 - mae: 0.0367 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 121/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1772 - mae: 0.0365 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 122/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1772 - mae: 0.0387 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 123/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1767 - mae: 0.0362 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 124/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1798 - mae: 0.0399 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 125/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1773 - mae: 0.0365 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 126/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1837 - mae: 0.0444 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 127/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1806 - mae: 0.0421 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 128/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1810 - mae: 0.0416 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 129/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1757 - mae: 0.0357 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 130/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1801 - mae: 0.0406 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 131/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1832 - mae: 0.0437 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 132/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1816 - mae: 0.0431 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 133/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1817 - mae: 0.0424 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 134/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1807 - mae: 0.0404 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 135/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1818 - mae: 0.0422 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 136/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1812 - mae: 0.0414 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 137/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1779 - mae: 0.0377 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 138/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1754 - mae: 0.0346 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 139/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1805 - mae: 0.0407 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 140/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1814 - mae: 0.0422 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 141/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1854 - mae: 0.0458 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 142/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1820 - mae: 0.0431 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 143/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1824 - mae: 0.0429 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 144/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1787 - mae: 0.0385 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 145/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1765 - mae: 0.0361 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 146/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1781 - mae: 0.0378 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 147/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1830 - mae: 0.0435 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 148/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1788 - mae: 0.0404 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 149/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1800 - mae: 0.0403 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 150/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1823 - mae: 0.0432 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 151/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1838 - mae: 0.0449 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 152/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1750 - mae: 0.0342 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 153/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1802 - mae: 0.0406 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 154/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1807 - mae: 0.0413 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 155/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1766 - mae: 0.0360 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 156/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1772 - mae: 0.0378 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 157/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1806 - mae: 0.0419 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 158/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1811 - mae: 0.0408 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 159/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1811 - mae: 0.0417 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 160/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1791 - mae: 0.0412 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 161/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1839 - mae: 0.0448 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 162/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1794 - mae: 0.0395 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 163/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1782 - mae: 0.0383 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 164/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1815 - mae: 0.0437 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 165/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1830 - mae: 0.0440 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 166/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1799 - mae: 0.0395 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 167/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1822 - mae: 0.0425 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 168/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1767 - mae: 0.0359 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 169/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1799 - mae: 0.0416 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 170/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1836 - mae: 0.0442 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 171/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1779 - mae: 0.0381 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 172/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1787 - mae: 0.0383 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 173/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1805 - mae: 0.0408 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 174/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1756 - mae: 0.0352 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 175/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1760 - mae: 0.0355 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 176/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1820 - mae: 0.0428 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 177/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1764 - mae: 0.0361 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 178/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1769 - mae: 0.0370 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 179/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1797 - mae: 0.0416 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 180/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1769 - mae: 0.0363 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 181/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1788 - mae: 0.0392 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 182/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1803 - mae: 0.0416 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 183/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1838 - mae: 0.0448 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 184/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1770 - mae: 0.0365 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 185/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1827 - mae: 0.0430 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 186/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1803 - mae: 0.0407 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 187/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1853 - mae: 0.0465 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 188/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1822 - mae: 0.0423 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 189/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1821 - mae: 0.0422 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 190/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1782 - mae: 0.0395 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 191/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1772 - mae: 0.0370 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 192/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1805 - mae: 0.0404 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 193/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1742 - mae: 0.0345 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 194/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1771 - mae: 0.0369 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 195/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1825 - mae: 0.0429 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 196/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1772 - mae: 0.0386 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 197/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1765 - mae: 0.0360 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 198/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1835 - mae: 0.0444 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 199/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1797 - mae: 0.0399 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 200/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1817 - mae: 0.0420 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 201/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1840 - mae: 0.0452 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 202/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1785 - mae: 0.0381 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 203/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1805 - mae: 0.0414 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 204/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1814 - mae: 0.0417 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 205/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1816 - mae: 0.0424 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 206/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1785 - mae: 0.0383 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 207/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1815 - mae: 0.0418 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 208/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1753 - mae: 0.0348 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 209/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1815 - mae: 0.0417 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 210/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1800 - mae: 0.0394 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 211/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1792 - mae: 0.0403 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 212/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1824 - mae: 0.0428 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 213/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1815 - mae: 0.0427 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 214/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1808 - mae: 0.0406 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 215/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1794 - mae: 0.0399 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 216/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1808 - mae: 0.0405 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 217/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1858 - mae: 0.0476 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 218/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1813 - mae: 0.0421 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 219/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1778 - mae: 0.0376 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 220/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1798 - mae: 0.0402 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 221/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1802 - mae: 0.0407 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 222/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1796 - mae: 0.0401 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 223/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1791 - mae: 0.0391 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 224/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1852 - mae: 0.0467 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 225/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1846 - mae: 0.0456 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 226/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1821 - mae: 0.0423 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 227/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1829 - mae: 0.0455 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 228/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1840 - mae: 0.0449 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 229/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1809 - mae: 0.0418 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 230/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1837 - mae: 0.0452 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 231/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1780 - mae: 0.0379 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 232/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1791 - mae: 0.0390 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 233/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1792 - mae: 0.0397 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 234/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1823 - mae: 0.0422 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 235/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1767 - mae: 0.0366 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 236/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1803 - mae: 0.0411 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 237/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1777 - mae: 0.0371 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 238/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1817 - mae: 0.0421 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 239/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1830 - mae: 0.0439 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 240/250\n","11/11 [==============================] - 2s 208ms/step - loss: 0.1810 - mae: 0.0416 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 241/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1846 - mae: 0.0455 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 242/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1805 - mae: 0.0409 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 243/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1787 - mae: 0.0400 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 244/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1798 - mae: 0.0400 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 245/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1805 - mae: 0.0405 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 246/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1779 - mae: 0.0374 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 247/250\n","11/11 [==============================] - 2s 207ms/step - loss: 0.1807 - mae: 0.0415 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 248/250\n","11/11 [==============================] - 2s 209ms/step - loss: 0.1784 - mae: 0.0389 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 249/250\n","11/11 [==============================] - 2s 210ms/step - loss: 0.1860 - mae: 0.0472 - val_loss: 0.1745 - val_mae: 0.0348\n","Epoch 250/250\n","11/11 [==============================] - 2s 206ms/step - loss: 0.1844 - mae: 0.0466 - val_loss: 0.1745 - val_mae: 0.0348\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4IJvyo-t1en"},"source":["model.save(directory + '/weightsRegressor/vgg_model_pretrained.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"59F3c_a3yfZ1"},"source":["def gather_four_coords(points):\n","  results = [(round(points[i] * x_train.shape[2]), round(points[i + 1] * x_train.shape[1])) for i in range(0, len(points), 2)] + [(round(points[0] * x_train.shape[2]), round(points[1] * x_train.shape[1]))]\n","  return results\n","\n","predicted_coords = model.predict(x_test/255, batch_size = 2, verbose = 0)\n","for i, image in enumerate(x_test):\n","  image = Image.fromarray(image)\n","  points = gather_four_coords(predicted_coords[i])\n","  draw = ImageDraw.Draw(image)\n","  draw.line(points, width = 5, fill=\"yellow\")\n","  image.save(directory + f\"regressor_predicted/vgg_results/{i}.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvDzOQLXFliV"},"source":["Итоги:\n","* Нужно обучать на большом количестве эпох\n","* resnet не даёт более хорошие результаты\n","* Dropout в свёрточных слоях ухудшает результаты\n","* binary_crossentropy быстрее обучается"]}]}